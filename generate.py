"""Generate core of static thr

Experiment with allowing variable base
"""

from gzip import open as gzip_open
from json import load as json_load, dump as json_dump
from mako.template import Template
from mako import exceptions
from os import makedirs
import os.path as osp
from itertools import groupby as itertools_groupby
from shutil import copyfileobj
from re import findall as regex_findall, IGNORECASE, split as regex_split, match as regex_match, escape as regex_escape
from requests import Session
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
from nltk import download as nltk_download, pos_tag, word_tokenize
from contractions import fix as contractions_fix
from pandas import DataFrame
import myArgs
from math import ceil, log
from sqlitedict import SqliteDict
from copypage import CopyPage
from string import punctuation
from spellchecker import SpellChecker
import bookspellchecker

args = myArgs.Parse(
    base=16,  # base to encode books in
    Nselect=100,  # number of books to select from unreviewed and reviewed
    # minimum number of pages a book can have to be selected (overwritten if book is reviewed)
    minPages=6,
    # maximum number of pages a book can have to be selected (overwritten if book is reviewed)
    maxPages=20,
    out="dist",  # the folder to which to generate the static THR
    # query to search for in title, author, and pages (e.g. DLM is an author)
    query="",
    hasCat=True,  # whether to only select books that have a category
    hasAudience=True,  # whether to only select books that have an audience
    lang="en",  # language of books to select (default English)
    # minimum number of appearances a word can have to qualify for index selection
    minAppearances=2,
    # minimum number of words given a book can have in the index selection process
    minWordsPerBook=8,
    # maximum number of works a given book can have in the index selection process,
    maxWordsPerBook=100,
    spellcheck=False,  # whether to filter out books with spelling errors
    spellcheckdata=False,  # whether to generate information about spelling errors
    # mode for book spell checking (now only simple, complex)
    spellcheckmode='simple',
    # where to put a summary of misspelled words
    spellcheckwrongout='data/misspelledwords.txt',
    spellcheckcorrectout='data/correctwords.txt',
    images="/archives/tarheelreader/production",  # folder for images
    # where to find the book archive (generated by using data\fetchBooks)
    books="data/books.json.gz",
    collections="data/collections.json.gz"
)

cp = CopyPage()

# get all the books
books = json_load(gzip_open(args.books, "rt", encoding="utf-8"))


def render(template, view):
    """Render a template with traceback"""
    try:
        html = Template(template).render(**view)
    except Exception:
        print(exceptions.text_error_template().render())
        raise
    return html


def matchesQuery(book, query):
    """True if the query occurs in the book"""
    return (
        query in book["author"]
        or query in book["title"]
        or any(query in page["text"] for page in book["pages"])
    )


# get the books that qualify
books = [
    book
    for book in books
    # is the specified language (English default)
    if book["language"] == args.lang
    # satisfies the query if any
    and (not args.query or matchesQuery(book, args.query))
    # is categorized
    and (not args.hasCat or len(book["categories"]) > 0)
    # has an audience
    and (not args.hasAudience or book["audience"] in "EC")
    # has enough pages or is reviewed
    and ((args.minPages <= len(book["pages"]) <= args.maxPages) or book["reviewed"])
]

# get stop words (the, is, are, etc.)
nltk_download('stopwords')
stop_words = set(stopwords.words('english'))

spell = SpellChecker()


if args.spellcheck:
    book_spell = bookspellchecker.BookSpellCheck(
        spellcheckdata=args.spellcheckdata, stop_words=stop_words)
    books, set_unknown, set_known = book_spell.spellcheck(
        books, mode=args.spellcheckmode)

    if args.spellcheckdata:
        print(
            f'There were {len(set_unknown)} words that were not spelled correctly', flush=True)
        print(f'There were {len(set_known)} words spelled correctly')
        # write out a file
        with open(args.spellcheckwrongout, "wt", encoding="utf-8") as fp:
            fp.write("\n".join(sorted(set_unknown)))

        with open(args.spellcheckcorrectout, "wt", encoding="utf-8") as fp:
            fp.write('\n'.join(sorted(set_known)))


print('Number of books that qualify: ', len(books))

# break into reviewed and unreviewed
reviewed = [book for book in books if book["reviewed"]][: args.Nselect]
unreviewed = [book for book in books if not book["reviewed"]][: args.Nselect]
# reviewed books come first
selected = reviewed + unreviewed

# spell = Spellchecker()

# activate stemmer
stemmer = PorterStemmer()


def getWords(book, stemmer):
    """Return words from the book

    replace contractions
    check spelling
    stem
    """
    words = []
    for page in book["pages"]:
        text = contractions_fix(page["text"])
        text = text.replace("'", "")
        words += [
            stemmer.stem(word).lower()
            for word in regex_findall(r"[a-z]+", text, flags=IGNORECASE)
            # spell.known(words=[word]) and
            if word.lower() not in stop_words
        ]

    return set(words)


index = []
for book in selected:
    slug = book["slug"]
    words = getWords(book, stemmer)
    for word in words:
        index.append((word, slug))
    for category in book["categories"]:
        index.append((category.upper(), slug))
    if book["audience"] == "C":
        index.append(("CAUTION", slug))


index = DataFrame(index, columns=['word', 'slug'])

print(f'(Words x Slugs): {index.shape[0]}')

# repeat because dropping some might change inclusion of others
slug_set = set(index.slug.unique())
word_set = set(index.word.unique())
while True:
    # drop the words that only occur a few times
    booksPerWord = index.groupby("word").slug.count()
    index = index[index.word.isin(
        booksPerWord[booksPerWord > args.minAppearances].index)]

    # drop the books that have too few or too many words
    wordsPerBook = index.groupby("slug").word.count()
    rightSize = (wordsPerBook >= args.minWordsPerBook) & (
        wordsPerBook < args.maxWordsPerBook)
    index = index[index.slug.isin(wordsPerBook[rightSize].index)]

    new_slug_set = set(index.slug.unique())
    new_word_set = set(index.word.unique())
    if len(new_slug_set) == len(slug_set) and len(new_word_set) == len(word_set):
        break
    slug_set = new_slug_set
    word_set = new_word_set

# make the index from words to slugs
wordToSlugs = index.groupby("word").slug.apply(list)
slugs = index.slug.unique()

# only keep the selected books for the rest of the processing
books = [book for book in books if book["slug"] in slugs]
Nbooks = len(books)
Dbooks = int(ceil(log(Nbooks, args.base)))

print(
    f'Parsed out some books! Now have {Nbooks}, or {Dbooks} (in base {args.base})')

# count the pictures
pictures = set()
for book in books:
    for page in book["pages"]:
        pictures.add(page["url"])
Npictures = len(pictures)
Dpictures = int(ceil(log(Npictures, args.base)))

print(f'Number of pictures: {Npictures}, or {Dpictures} (in base {args.base})')

OUT = args.out
CONTENT = osp.join(OUT, "content")


def make_pageid(i):
    """return the fragment for the page"""
    return f"p{i}"


# these must be in collation order
encoding = "0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"


def encode(value, digits):
    """Encode an integer into a string"""
    r = []
    base = args.base
    for _ in range(digits):
        r.append(encoding[value % base])
        value //= base
    return "".join(r[::-1])


# map slugs to ids
bookmap = {}


def make_bookid(slug):
    """get unique id for a book"""
    if slug not in bookmap:
        num_books = len(bookmap)
        res_encode = encode(num_books, Dbooks)
        # *list makes the file structure first_digit/second_digit/...last_digit.html
        path = osp.join(CONTENT, *list(res_encode)) + ".html"
        bookmap[slug] = (res_encode, path)
    return bookmap[slug]


# map image URL to new name
makedirs(OUT, exist_ok=True)
imagemap = SqliteDict(osp.join(OUT, "imagemap.sd"), autocommit=True)


def imgurl(url, bid, bpath, session):
    """localize and return full image url for a picture"""
    if url in imagemap:
        path = imagemap[url]
    else:
        num_pictures = len(imagemap)
        res_encode = encode(num_pictures, Dpictures)
        path = osp.join(CONTENT, *res_encode) + ".jpg"
        makedirs(osp.dirname(path), exist_ok=True)
        try:
            resp = session.get("http://tarheelreader.org" + url, stream=True)
        except:
            return ""
        with open(path, 'wb') as f:
            resp.raw.decode_content = True
            copyfileobj(resp.raw, f)
        imagemap[url] = path
    return path, osp.relpath(path, osp.dirname(bpath))


# write the books copying the images
ndx = []
template = open("src/book.mako").read()
lastReviewed = None

book_css = osp.join(OUT, cp.copy("book.css"))
book_js = osp.join(OUT, cp.link("book.js"))

progress_counter = len(books)//10
slugs_not_found = set()
for progress, book in enumerate(books):
    bid, bpath = make_bookid(book["slug"])

    books[progress]['id'] = bid
    books[progress]['link'] = bpath

    if progress % progress_counter == 0:
        print(
            f"Template making progress: {progress}, book ID {bid}, book path {bpath}")
    icons = []
    if book["audience"] == "C":
        icons.append("C")
    if book["reviewed"]:
        icons.append("R")
        lastReviewed = bid
    last = bid
    ipath = osp.join(osp.dirname(bpath), "index.html")
    sess = Session()

    title_image_path, title_image = imgurl(book['pages'][0]['url'], bid, bpath, sess)
    if not title_image:
        slugs_not_found.add(book['slug'])
        continue

    books[progress]['image'] = title_image_path

    ndx.append(
        dict(
            title=book["title"],
            author=book["author"],
            pages=len(book["pages"]),
            image=title_image,
            icons=" ".join(icons),
            id=bid,
            link=bid[-1],
            path=ipath,
        )
    )
    view = dict(start="#" + make_pageid(1),
                title=book["title"], index=f"./#{bid}")

    _, second_image = imgurl(book['pages'][1]['url'], bid, bpath, sess)
    if not second_image:
        slugs_not_found.add(book['slug'])
        continue

    pages = [
        dict(
            title=book["title"],
            author=book["author"],
            image=imgurl(book["pages"][1]["url"], bid, bpath, sess)[1],
            id=make_pageid(1),
            back=view["index"],
            next="#" + make_pageid(2),
        )
    ]

    image_not_found = False
    for i, page in enumerate(book["pages"][1:]):
        pageno = i + 2
        curr_image = imgurl(page['url'], bid, bpath, sess)[1]

        if not curr_image:
            slugs_not_found.add(book['slug'])
            image_not_found = True
            break

        pages.append(
            dict(
                pageno=pageno,
                id=make_pageid(pageno),
                image=curr_image,
                text=page["text"],
                back="#" + make_pageid(pageno - 1),
                next="#" + make_pageid(pageno + 1),
            )
        )

    if image_not_found:
        continue

    pages[-1]["next"] = "#done"
    view["pages"] = pages
    view["bid"] = bid
    view["css"] = osp.relpath(book_css, osp.dirname(bpath))
    view["js"] = osp.relpath(book_js, osp.dirname(bpath))

    html = render(template, view)
    makedirs(osp.dirname(bpath), exist_ok=True)
    with open(bpath, "wt", encoding="utf-8") as fp:
        fp.write(html)

books = [book for book in books if book['slug'] not in slugs_not_found]

books_by_title = sorted(books, key=lambda k: k['title'])
books_by_author = sorted(books, key=lambda k: k['author'])

for book in books:
    book['average_rating'] = book['rating_total'] / \
        book['rating_count'] if book['rating_count'] > 0 else 0

books_by_rating = sorted(books, key=lambda k: k['average_rating'])
books_by_rating_count = sorted(books, key=lambda k: k['rating_total'])


def get_id_for_book(k): return bookmap[k['slug']][0]


books_by_title = list(map(get_id_for_book, books_by_title))
books_by_author = list(map(get_id_for_book, books_by_author))
books_by_rating = list(map(get_id_for_book, books_by_rating))
books_by_rating_count = list(map(get_id_for_book, books_by_rating_count))

# write the sort indexes
SORTOUT = osp.join(CONTENT, "sort")
makedirs(SORTOUT, exist_ok=True)
with open(osp.join(SORTOUT, "title"), "wt", encoding="utf-8") as fp:
    fp.write(''.join(books_by_title))

with open(osp.join(SORTOUT, "author"), "wt", encoding="utf-8") as fp:
    fp.write(''.join(books_by_author))

with open(osp.join(SORTOUT, "rating"), "wt", encoding="utf-8") as fp:
    fp.write(''.join(books_by_rating))

with open(osp.join(SORTOUT, "ratingcount"), "wt", encoding="utf-8") as fp:
    fp.write(''.join(books_by_rating_count))

print("Last Reviewed", lastReviewed)


def filter_slugs(book_slugs_collection, book_slugs_given):
    return any([b in book_slugs_given for b in book_slugs_collection])


# work on collections
collections = json_load(gzip_open(args.collections, "rt", encoding="utf-8"))
collections = {key: value for key, value in collections.items() if filter_slugs(
    value['book_slugs'], bookmap.keys())}
for slug in collections:
    collections[slug]['book_slugs'] = list(
        filter(lambda k: k in bookmap.keys(), collections[slug]['book_slugs']))

collection_template = open('src/collections.mako').read()
collection_path = osp.join(OUT, "collections")

collection_css = osp.join(OUT, cp.copy("collections.css"))

makedirs(collection_path, exist_ok=True)

all_collections = [key + ',' + '-'.join(item['title'].split(' ')) for key, item in collections.items()]
for slug, val in collections.items():
    path = osp.join(collection_path, slug + '.html')
    book_collection = []
    for book in [book for book in books if book['slug'] in val['book_slugs']]:
        book_collection.append(
            dict(
                id=book['id'],
                link=book['link'],
                title=book['title'],
                author=book['author'],
                image=book['image']
            )
        )

    for b in book_collection:
        b['link'] = osp.relpath(b['link'], path)
        b['image'] = osp.relpath(b['image'], path)

    view = dict(
        name=val['title'],
        books=book_collection,
        back='/',
        css=osp.relpath(collection_css, path)
    )
    with open(path, 'wt', encoding='utf-8') as fp:
        fp.write(render(collection_template, view))

# write out an all collections file
with open(osp.join(collection_path, 'ALL'), 'wt', encoding='utf-8') as fp:
    fp.write(' '.join(all_collections))

# write the index.htmls
idxtemplate = open("src/book-index.mako").read()
idxpaths = sorted(set(b["path"] for b in ndx))
start = osp.join(CONTENT, "index.html")
back = start
i = 1
for path, group in itertools_groupby(ndx, lambda v: v["path"]):
    view = dict(
        name="index",
        books=group,
        back=osp.relpath(back, osp.dirname(path)),
        next=osp.relpath(idxpaths[i] if i < len(idxpaths)
                         else start, osp.dirname(path)),
        css=osp.relpath(osp.join(OUT, "index.css"), path),
    )
    with open(path, "wt", encoding="utf-8") as fp:
        fp.write(render(idxtemplate, view))
    back = path
    i += 1

# write the word indexes
WOUT = osp.join(CONTENT, "index")
makedirs(WOUT, exist_ok=True)

for word, slugs in wordToSlugs.iteritems():
    if len(word) < 3:
        continue
    # bookmap[slug] is a tuple whose structure is (book ID, book path)
    ids = sorted([bookmap[slug][0] for slug in slugs])
    with open(osp.join(WOUT, word), "wt", encoding="utf-8") as fp:
        fp.write("".join(ids))

all_words = ' '.join(filter(lambda word: word.upper()
                            != word, wordToSlugs.keys()))
print(f'All Words: {all_words}')
with open(osp.join(WOUT, "ALLWORDS"), "wt", encoding="utf-8") as fp:
    fp.write(all_words)

# make sure CAUTION exists
with open(osp.join(WOUT, "CAUTION"), "at", encoding="utf-8") as fp:
    fp.write("")

# write the AllAvailable file
with open(osp.join(WOUT, "AllAvailable"), "wt", encoding="utf-8") as fp:
    # first-last
    fp.write("%s-%s" % ("0" * Dbooks, last))

# write out a list of the images for possible prefetch...
with open(osp.join(CONTENT, "images.json"), "wt", encoding="utf-8") as fp:
    json_dump([osp.relpath(path, OUT) for path in imagemap.values()], fp)

# record parameters needed by the js
config = {
    "base": args.base,
    "digits": Dbooks,
    "first": "0" * Dbooks,
    "lastReviewed": lastReviewed,
    "last": last,
}
print(f'Configuration Parameters: {config}')
with open(osp.join(CONTENT, "config.json"), "wt") as fp:
    json_dump(config, fp)
